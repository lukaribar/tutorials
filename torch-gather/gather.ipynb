{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to use torch.gather (and how not to...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook provides a short tutorial on using `torch.gather` through a few examples, while showcasing some common pitfalls and differences from the `numpy` behaviour."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(100)\n",
    "np.random.seed(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic usage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first look at a basic example: Starting from a 2D matrix `(m, n)`, construct a new 2D matrix `(m, l)`, gathering elements across rows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll start from a 3x3 matrix containing numbers from 0 to 9."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0, 1, 2],\n",
      "        [3, 4, 5],\n",
      "        [6, 7, 8]])\n"
     ]
    }
   ],
   "source": [
    "# 2D matrix, gather across dim=1\n",
    "x = torch.arange(9).reshape(3, 3)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's construct a 3x4 matrix, gathering the following numbers from each row `[[2, 2, 0, 0],[3, 4, 5, 5], [6, 6, 7, 7]]`. For each row we therefore specify the corresponding indices:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2, 2, 0, 0],\n",
       "        [3, 4, 5, 5],\n",
       "        [6, 6, 7, 7]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For each row, specify which index along the row we want to take\n",
    "torch.gather(x, dim=1, index=torch.tensor([[2, 2, 0, 0], [0, 1, 2, 2], [0, 0, 1, 1]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What if we want to collect the same indices for each row, e.g. `[2, 2, 0, 0], [5, 5, 3, 3], [8, 8, 6, 6]`? The easiest thing to do is to again explicity specify indices for each row, same as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2, 2, 0, 0],\n",
       "        [5, 5, 3, 3],\n",
       "        [8, 8, 6, 6]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Easiest thing to do -> explicty specify for each row again, same as before.\n",
    "torch.gather(x, dim=1, index=torch.tensor([[2, 2, 0, 0], [2, 2, 0, 0], [2, 2, 0, 0]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What happens if we only specify the indices for one row? We might expect that the indices will be broadcast, so that the specified indices will be selected for each row. But that's not what happens!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2, 2, 0, 0]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.gather(x, dim=1, index=torch.tensor([[2, 2, 0, 0]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have only gathered the elements for the first row! Similarly, if we specify indices for two rows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2, 2, 0, 0],\n",
       "        [5, 5, 3, 3]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Similar if two rows are specified in the index\n",
    "torch.gather(x, dim=1, index=torch.tensor([[2, 2, 0, 0], [2, 2, 0, 0]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the first case, the tensor is actually size `(1, 4)`, instead of the expected `(3, 4)`. This is because `torch.gather` does not broadcast `index` against `input`, it just expects that for each dimension `d != dim`, `index.size(d) <= input.size(d)`. In this case, `index.size(0) = 1`, so the output will gather only across the first row."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to achieve the desired behaviour, we need to broadcast manually:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2, 2, 0, 0],\n",
       "        [5, 5, 3, 3],\n",
       "        [8, 8, 6, 6]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Alternative, broadcast manually\n",
    "torch.gather(x, dim=1, index=torch.tensor([[2, 2, 0, 0]]).expand(3, -1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why could this be annoying? `numpy` behaviour is different! `numpy` **does** broadcast `index` against `input`, so it will *only* work if, for each dimension `d != dim`, `index.size(d) = input.size(d)`, or `index.size(d) = 1` and is broadcast against `input`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2, 2, 0, 0],\n",
       "       [5, 5, 3, 3],\n",
       "       [8, 8, 6, 6]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# numpy will broadcast indices against input\n",
    "x = np.arange(9).reshape(3, 3)\n",
    "np.take_along_axis(x, indices=np.array([[2, 2, 0, 0]]), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This does not work!\n"
     ]
    }
   ],
   "source": [
    "# This breaks as shapes are not matching for the non-gather dimensions!\n",
    "x = np.arange(9).reshape(3, 3)\n",
    "try:\n",
    "    np.take_along_axis(x, indices=np.array([[2, 2, 0, 0], [2, 2, 0, 0]]), axis=1)\n",
    "except:\n",
    "    print(\"This does not work!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conclusion**: `index` needs to be **exactly** the same shape as `output`, each non-gather dimension needs to be smaller or equal than the corresponding `input` dimension, while the dimension across which we are gathering can have an arbitrary size."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Broadcasting `gather`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can adapt `torch.gather` to instead work similarly to `numpy.take_along_axis`, which is usually the more useful behaviour!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gather(input: torch.Tensor, dim: int, index: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"torch.gather with broadcasting\"\"\"\n",
    "    dim = (dim < 0) * input.ndim + dim\n",
    "    return torch.gather(\n",
    "        input,\n",
    "        dim,\n",
    "        index.expand(*input.shape[:dim], index.shape[dim], *input.shape[dim + 1 :]),\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the behaviour on our previous example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2, 2, 0, 0],\n",
       "        [5, 5, 3, 3],\n",
       "        [8, 8, 6, 6]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.arange(9).reshape(3, 3)\n",
    "gather(x, dim=1, index=torch.tensor([[2, 2, 0, 0]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *Batched* multi-dimensional gather"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far, the tutorial has looked at a basic 2D example. Let's now look at a more generic multi-dimensional gather operation and a common scenario where we would like to gather elements across a *batched* tensor.\n",
    "\n",
    "Specifically, we would like to look at the following. Starting from an input tensor `x`, specified dimension `dim` and `idx` tensor of shape `idx.shape = x.shape[:dim] + (k,)` where each element is between `0` and `x.shape[dim] - 1`, we want to construct the output tensor `y` of shape `y.shape = idx.shape + x.shape[dim + 1:]`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at an example. We have a 6D input tensor `x` of shape `(2, 3, 5, 7, 11, 13)`. We consider the first three dimensions `(2, 3, 5)` of the input as the *batch* dimensions, so that each batch element is a tensor of shape `(7, 11, 13)`. Here, `7` is the sequence dimension across we would like to gather \"elements\", where each \"element\" is a matrix of size `(11, 13)`.\n",
    "\n",
    "For each batch, we want to take the *same* 4 \"elements\" of the 7, so that the final output shape should be `(2, 3, 5, 4, 11, 13)`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will take a look at a few different ways of achieving the same behaviour."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gather across dim=3, where (2, 3, 5) are batch dimensions,\n",
    "# 7 is the sequence dimension with each element being (11, 13) matrix\n",
    "x = torch.randn(2, 3, 5, 7, 11, 13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batched gather across dim=3, choose 4 out of 7\n",
    "# We want output shape (2, 3, 5, 4, 11, 13)\n",
    "idx = torch.randint(7, (2, 3, 5, 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `torch.gather`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's achieve this using `torch.gather`. We will also measure the time to get an idea of how long each method takes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we've seen before, the `index` and `output` shapes of `torch.gather` are *exactly* the same. This means that we need to manually broadcast dimensions of our `idx` to get the desired output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken:  0.00049591064453125\n",
      "Output shape:  torch.Size([2, 3, 5, 4, 11, 13])\n"
     ]
    }
   ],
   "source": [
    "t0 = time.time()\n",
    "# NOTE: idx.shape needs to be the same as y1.shape, so broadcast the last two dimensions\n",
    "y1 = torch.gather(x, dim=3, index=idx[..., None, None].expand(*idx.shape, *x.shape[4:]))\n",
    "print(\"Time taken: \", time.time() - t0)\n",
    "print(\"Output shape: \", y1.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to be careful here! If we did not explicitly expand the `index`, the operation would work, but would not give us our desired output!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrong output shape: torch.Size([2, 3, 5, 4, 1, 1])\n"
     ]
    }
   ],
   "source": [
    "# Possible gotcha: forget to expand, or expand incorrectly (works as long as dim(idx) <= dim(x) for all non-gather dimensions)\n",
    "y1_wrong = torch.gather(x, dim=3, index=idx[..., None, None]) # this works, but slices last two dimensions\n",
    "print(\"Wrong output shape:\", y1_wrong.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This method only gets the first row/column of each element in the sequence\n",
    "torch.allclose(y1_wrong, y1[..., :1, :1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course, we can instead use our own broadcasting version and check it gives the expected answer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.allclose(gather(x, dim=3, index=idx[..., None, None]), y1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Advanced indexing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of using `torch.gather`, we can use advanced indexing to achieve the same behaviour. For this, we need to construct `idx0`, `idx1`, `idx2`, and `idx3` for each of the first four dimensions, so that:\n",
    "\n",
    "`y[i, j, k, l] = x[idx0[i, j, k, l], idx1[i, j, k, l], idx2[i, j, k, l], idx3[i, j, k, l]]`.\n",
    "\n",
    "Each of the index tensors has the same shape as the output's first four dimensions, i.e. `y.shape[:4]`.\n",
    "\n",
    "In this case, `idx3` will be the `idx` tensor we have constructed previously, while the other indices just need to indicate that we are selecting all of the dimensions. For this, we use broadcasting for the non-gather batch dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken:  0.0014488697052001953\n"
     ]
    }
   ],
   "source": [
    "t0 = time.time()\n",
    "y2 = x[\n",
    "    torch.arange(2)[:, None, None, None],\n",
    "    torch.arange(3)[None, :, None, None],\n",
    "    torch.arange(5)[None, None, :, None],\n",
    "    idx,\n",
    "]\n",
    "print(\"Time taken: \", time.time() - t0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that this takes around an order of magnitude more than the method using `gather`! Let's check that we got the same result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.allclose(y1, y2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Flattening before indexing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of constructing three indices for the batch dimensions (`idx0`, `idx1`, `idx2`), we can flatten these dimensions before indexing. This however does require a few annoying reshaping operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken:  0.0012068748474121094\n"
     ]
    }
   ],
   "source": [
    "t0 = time.time()\n",
    "y3 = x.flatten(end_dim=2)[\n",
    "    torch.arange(2 * 3 * 5)[:, None], idx.flatten(end_dim=2)\n",
    "].reshape(2, 3, 5, 4, 11, 13)\n",
    "print(\"Time taken: \", time.time() - t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.allclose(y1, y3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a similar approach, we could instead flatten the dimension across which we are gathering together with the batched dimensions. In this case, we need to ensure that the index has the correct offset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flatten gather dim=3 as well, calculate correct offsets\n",
    "offset = torch.arange(2 * 3 * 5).reshape(2, 3, 5, 1) * 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0008790493011474609\n"
     ]
    }
   ],
   "source": [
    "t0 = time.time()\n",
    "y4 = x.flatten(end_dim=3)[(idx + offset).flatten()].reshape(2, 3, 5, 4, 11, 13)\n",
    "print(time.time() - t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.allclose(y1, y4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When gathering elements from a multi-dimensional tensor, `torch.gather` is the fastest method. There are however a few important things to keep in mind:\n",
    "\n",
    "* `torch.gather` *does not* broadcast across non-gather dimensions. This means that we need to ensure that the `idx` tensor has the **same** shape as the desired output.\n",
    "* If the size of a non-gather dimension in `idx` is not the same as in the input tensor, `torch.gather` will still work as long as the size is smaller than the corresponding dimension in the input tensor. This is usually not what we want though! (as that dimension will be sliced)\n",
    "* For most commmon use cases, we can implement our own modified version of `torch.gather` that performs broadcasting.\n",
    "\n",
    "Note, this notebook only covered `torch.gather`, but all of the conclusions and broadcasting gotchas can be applied to `torch.scatter` as well."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
